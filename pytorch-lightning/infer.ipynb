{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "ROOT_DIR = Path(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "BASE_DIR = ROOT_DIR / \"pytorch-lightning\"\n",
    "sys.path.append(ROOT_DIR)\n",
    "sys.path.append(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR = ROOT_DIR / \"working\"\n",
    "CONFIG_DIR = BASE_DIR / \"config\"\n",
    "\n",
    "COMPETITION_DATA_DIR = INPUT_DIR / \"hubmap-organ-segmentation\"\n",
    "\n",
    "CONFIG_YAML_PATH = CONFIG_DIR / \"default.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Union, Optional, Tuple, Mapping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yaml\n",
    "import logging \n",
    "import hashlib\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from ast import literal_eval\n",
    "from multimethod import multimethod\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "from rasterio.windows import Window\n",
    "from torchvision import transforms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from segmentation_models_pytorch.base import modules as md\n",
    "from fastai.vision.all import PixelShuffle_ICNR\n",
    "\n",
    "from timm.optim import create_optimizer_v2\n",
    "from pl_bolts.optimizers import lr_scheduler\n",
    "from losses_metrics import SymmetricLovaszLoss, Dice_soft, Dice_threshold, Dice_soft_func, Dice_threshold_func\n",
    "\n",
    "from segmentation_models_pytorch.decoders.deeplabv3.decoder import ASPP\n",
    "from segmentation_models_pytorch.decoders.fpn.decoder import FPNDecoder\n",
    "from segmentation_models_pytorch.decoders.unetplusplus.decoder import CenterBlock\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "from segmentation_models_pytorch.base import (\n",
    "    SegmentationModel,\n",
    "    SegmentationHead,\n",
    "    ClassificationHead,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "sz = 512    # the size of tiles\n",
    "reduce = 4  # reduce the original images by 4 times\n",
    "TH = 0.225  # threshold for positive predictions\n",
    "DATA = './input/hubmap-organ-segmentation/test_images/'\n",
    "TRAIN_CSV = \"./input/hubmap-organ-segmentation/train.csv\"\n",
    "TEST_CSV = './input/hubmap-organ-segmentation/test.csv'\n",
    "# MODELS = [f'./input/hubmap-models/hubmap_models/run_0/model_{i}.pth' for i in range(4)] + [f'../input/hubmap-models/hubmap_models/run_1/model_{i}.pth' for i in range(4)] + [f'../input/training-fastai-baseline/model_{i}.pth' for i in range(4)]\n",
    "df_sample = pd.read_csv('./input/hubmap-organ-segmentation/sample_submission.csv')\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/thedevastator/hubmap-2022-256x256\n",
    "mean = np.array([0.7720342, 0.74582646, 0.76392896])\n",
    "std = np.array([0.24745085, 0.26182273, 0.25782376])\n",
    "\n",
    "s_th = 40  #saturation blancking threshold\n",
    "p_th = 1000*(sz//256)**2 #threshold for the minimum number of pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle2mask(mask_rle, shape=(1600,256)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (width,height) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args, printer=logging.info):\n",
    "    printer(\"==========       args      =============\")\n",
    "    for arg, content in args.__dict__.items():\n",
    "        printer(\"{}:{}\".format(arg, content))\n",
    "    printer(\"==========     args END    =============\")\n",
    "\n",
    "\n",
    "class EasyConfig(dict):\n",
    "    def __getattr__(self, key: str) -> Any:\n",
    "        if key not in self:\n",
    "            raise AttributeError(key)\n",
    "        return self[key]\n",
    "\n",
    "    def __setattr__(self, key: str, value: Any) -> None:\n",
    "        self[key] = value\n",
    "\n",
    "    def __delattr__(self, key: str) -> None:\n",
    "        del self[key]\n",
    "\n",
    "    def load(self, fpath: str, *, recursive: bool = False) -> None:\n",
    "        \"\"\"load cfg from yaml\n",
    "\n",
    "        Args:\n",
    "            fpath (str): path to the yaml file\n",
    "            recursive (bool, optional): recursily load its parent defaul yaml files. Defaults to False.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(fpath):\n",
    "            raise FileNotFoundError(fpath)\n",
    "        fpaths = [fpath]\n",
    "        if recursive:\n",
    "            extension = os.path.splitext(fpath)[1]\n",
    "            while os.path.dirname(fpath) != fpath:\n",
    "                fpath = os.path.dirname(fpath)\n",
    "                fpaths.append(os.path.join(fpath, 'default' + extension))\n",
    "        for fpath in reversed(fpaths):\n",
    "            if os.path.exists(fpath):\n",
    "                with open(fpath) as f:\n",
    "                    self.update(yaml.safe_load(f))\n",
    "\n",
    "    def reload(self, fpath: str, *, recursive: bool = False) -> None:\n",
    "        self.clear()\n",
    "        self.load(fpath, recursive=recursive)\n",
    "\n",
    "    # mutimethod makes python supports function overloading\n",
    "    @multimethod\n",
    "    def update(self, other: Dict) -> None:\n",
    "        for key, value in other.items():\n",
    "            if isinstance(value, dict):\n",
    "                if key not in self or not isinstance(self[key], EasyConfig):\n",
    "                    self[key] = EasyConfig()\n",
    "                # recursively update\n",
    "                self[key].update(value)\n",
    "            else:\n",
    "                self[key] = value\n",
    "\n",
    "    @multimethod\n",
    "    def update(self, opts: Union[List, Tuple]) -> None:\n",
    "        index = 0\n",
    "        while index < len(opts):\n",
    "            opt = opts[index]\n",
    "            if opt.startswith('--'):\n",
    "                opt = opt[2:]\n",
    "            if '=' in opt:\n",
    "                key, value = opt.split('=', 1)\n",
    "                index += 1\n",
    "            else:\n",
    "                key, value = opt, opts[index + 1]\n",
    "                index += 2\n",
    "            current = self\n",
    "            subkeys = key.split('.')\n",
    "            try:\n",
    "                value = literal_eval(value)\n",
    "            except:\n",
    "                pass\n",
    "            for subkey in subkeys[:-1]:\n",
    "                current = current.setdefault(subkey, EasyConfig())\n",
    "            current[subkeys[-1]] = value\n",
    "\n",
    "    def dict(self) -> Dict[str, Any]:\n",
    "        configs = dict()\n",
    "        for key, value in self.items():\n",
    "            if isinstance(value, EasyConfig):\n",
    "                value = value.dict()\n",
    "            configs[key] = value\n",
    "        return configs\n",
    "\n",
    "    def hash(self) -> str:\n",
    "        buffer = json.dumps(self.dict(), sort_keys=True)\n",
    "        return hashlib.sha256(buffer.encode()).hexdigest()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        texts = []\n",
    "        for key, value in self.items():\n",
    "            if isinstance(value, EasyConfig):\n",
    "                seperator = '\\n'\n",
    "            else:\n",
    "                seperator = ' '\n",
    "            text = key + ':' + seperator + str(value)\n",
    "            lines = text.split('\\n')\n",
    "            for k, line in enumerate(lines[1:]):\n",
    "                lines[k + 1] = (' ' * 2) + line\n",
    "            texts.extend(lines)\n",
    "        return '\\n'.join(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuBMAPDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, sz=512, reduce=4, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.reduce = reduce\n",
    "        self.sz = reduce*sz\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = rasterio.open(os.path.join(DATA,f'{self.dataframe.loc[idx, [\"id\"]][0]}.tiff'), transform = rasterio.Affine(1, 0, 0, 0, 1, 0), num_threads='all_cpus')\n",
    "        # some images have issues with their format \n",
    "        # and must be saved correctly before reading with rasterio\n",
    "        if data.count != 3:\n",
    "            subdatasets = data.subdatasets\n",
    "            layers = []\n",
    "            if len(subdatasets) > 0:\n",
    "                for i, subdataset in enumerate(subdatasets, 0):\n",
    "                    layers.append(rasterio.open(subdataset))\n",
    "        pad0 = (self.sz - data.shape[0]%self.sz)%self.sz\n",
    "        pad1 = (self.sz - data.shape[1]%self.sz)%self.sz\n",
    "        n0max = (data.shape[0] + pad0)//self.sz\n",
    "        n1max = (data.shape[1] + pad1)//self.sz\n",
    "\n",
    "        if n0max*n1max !=1:\n",
    "            merged = []\n",
    "            for index, i in enumerate(range(n0max*n1max)):\n",
    "                n0,n1 = i//n1max, i%n1max\n",
    "\n",
    "                x0,y0 = -pad0//2 + n0*self.sz, -pad1//2 + n1*self.sz\n",
    "\n",
    "                p00,p01 = max(0,x0), min(x0+self.sz,data.shape[0])\n",
    "                p10,p11 = max(0,y0), min(y0+self.sz,data.shape[1])\n",
    "                img = np.zeros((self.sz,self.sz,3),np.uint8)\n",
    "                # mapping the loade region to the tile\n",
    "                if data.count == 3:\n",
    "                    img[(p00-x0):(p01-x0),(p10-y0):(p11-y0)] = np.moveaxis(data.read([1,2,3],\n",
    "                        window=Window.from_slices((p00,p01),(p10,p11))), 0, -1)\n",
    "                else:\n",
    "                    for i,layer in enumerate(layers):\n",
    "                        img[(p00-x0):(p01-x0),(p10-y0):(p11-y0),i] = layer.read(1,window=Window.from_slices((p00,p01),(p10,p11)))\n",
    "                \n",
    "                if self.reduce != 1:\n",
    "                    img = cv2.resize(img,(self.sz//reduce,self.sz//reduce), interpolation = cv2.INTER_AREA)\n",
    "                if self.transform is not None:\n",
    "                    img = self.transform(image=(img/255.0))\n",
    "                    # img = self.transform(image=(img/255.0 - mean)/std)\n",
    "                merged.append(img[\"image\"].permute(1,2,0))\n",
    "            merged = torch.vstack([torch.hstack([merged[0],merged[1]]), torch.hstack([merged[2],merged[3]])])\n",
    "            return (merged, 0, pad0, pad1, n0max, n1max)\n",
    "\n",
    "        else:\n",
    "            img = cv2.resize(np.asarray(data.read([1,2,3])).transpose(1,2,0),(self.sz//reduce,self.sz//reduce), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(image=(img/255.0))\n",
    "                # img = self.transform(image=(img/255.0 - mean)/std)\n",
    "            img = img[\"image\"].permute(1,2,0)\n",
    "            merged = torch.vstack([torch.hstack([img,img]), torch.hstack([img,img])])\n",
    "            return (merged, -1, pad0, pad1, n0max, n1max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_frame: pd.DataFrame,\n",
    "        spatial_size: int,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(ignore=\"data_frame\")\n",
    "\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "        self.train_transform, self.val_transform, self.test_transform = self._init_transforms()\n",
    "\n",
    "    def _init_transforms(self) -> Tuple[Callable, Callable, Callable]:\n",
    "        spatial_size = (self.hparams.spatial_size, self.hparams.spatial_size)\n",
    "        train_transform = A.Compose([A.HorizontalFlip(),\n",
    "                                     A.VerticalFlip(),\n",
    "                                     A.RandomRotate90(),\n",
    "                                     A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, border_mode=cv2.BORDER_REFLECT),\n",
    "                                     A.OneOf([A.OpticalDistortion(p=0.3),\n",
    "                                              A.GridDistortion(p=.1),A.PiecewiseAffine(p=0.3)], p=0.3),\n",
    "                                     A.OneOf([A.HueSaturationValue(10,15,10),\n",
    "                                              A.CLAHE(clip_limit=2),\n",
    "                                              A.RandomBrightnessContrast()], p=0.3),\n",
    "                                     A.Resize(height=spatial_size[0],width=spatial_size[1]),\n",
    "                                     ToTensorV2()])\n",
    "\n",
    "        val_transform = A.Compose([A.Resize(height=spatial_size[0],width=spatial_size[1]),\n",
    "                                   ToTensorV2()])\n",
    "\n",
    "        test_transform = A.Compose([A.Resize(height=spatial_size[0],width=spatial_size[1]),\n",
    "                                   ToTensorV2()])\n",
    "\n",
    "        return train_transform, val_transform, test_transform\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            pass\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = self._dataset(self.data_frame, transform=self.test_transform)\n",
    "\n",
    "    def _dataset(self, df: pd.DataFrame, transform: Callable) -> Dataset:\n",
    "        return HuBMAPDataset(dataframe=df, transform=transform)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.train_dataset, train=True, val=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.val_dataset, train=False, val=True)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.test_dataset)\n",
    "\n",
    "    def _dataloader(self, dataset: Dataset, train: bool = False, val: bool = False) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True if train and val else False,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True if train and val else False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_from_config(cfg):\n",
    "    if cfg.architecture=='unetplusplus-with-aspp-fpn':\n",
    "        return UnetPlusPlus_with_ASPP_FPN(cfg.backbone, segmentation_channels=cfg.segmentation_channels, atrous_rates=tuple(cfg.atrous_rates), classes=cfg.classes)\n",
    "    else:\n",
    "        return getattr(smp, cfg.architecture)(cfg.backbone)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            skip_channels,\n",
    "            out_channels,\n",
    "            use_batchnorm=True,\n",
    "            attention_type=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
    "\n",
    "        self.shuf = PixelShuffle_ICNR(in_channels, in_channels*2//2)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        # x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "        x = self.shuf(x)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x\n",
    "\n",
    "class UnetPlusPlus_with_ASPP_FPN_Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels,\n",
    "        decoder_channels,\n",
    "        aspp_out_channels,\n",
    "        segmentation_channels=32,\n",
    "        atrous_rates=(6,12,18),\n",
    "        n_blocks=5,\n",
    "        use_batchnorm=True,\n",
    "        attention_type=None,\n",
    "        center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        self.aspp = ASPP(encoder_channels[0], aspp_out_channels, atrous_rates)\n",
    "        self.fpn = FPNDecoder(tuple(list((encoder_channels[0],)+decoder_channels[:-1])[::-1]), segmentation_channels=segmentation_channels, encoder_depth=n_blocks, merge_policy=\"cat\")\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        self.in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        self.skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        self.out_channels = decoder_channels\n",
    "        if center:\n",
    "            self.center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n",
    "\n",
    "        blocks = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(layer_idx + 1):\n",
    "                if depth_idx == 0:\n",
    "                    in_ch = self.in_channels[layer_idx]\n",
    "                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1)\n",
    "                    out_ch = self.out_channels[layer_idx]\n",
    "                else:\n",
    "                    out_ch = self.skip_channels[layer_idx]\n",
    "                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1 - depth_idx)\n",
    "                    in_ch = self.skip_channels[layer_idx - 1]\n",
    "                blocks[f\"x_{depth_idx}_{layer_idx}\"] = DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n",
    "        blocks[f\"x_{0}_{len(self.in_channels)-1}\"] = DecoderBlock(\n",
    "            self.in_channels[-1], 0, self.out_channels[-1], **kwargs\n",
    "        )\n",
    "        self.blocks = nn.ModuleDict(blocks)\n",
    "        self.depth = len(self.in_channels) - 1\n",
    "\n",
    "    def forward(self, *features):\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "        features = (self.aspp(features[0]),)+features[1:]\n",
    "        # start building dense connections\n",
    "        dense_x = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(self.depth - layer_idx):\n",
    "                if layer_idx == 0:\n",
    "                    output = self.blocks[f\"x_{depth_idx}_{depth_idx}\"](features[depth_idx], features[depth_idx + 1])\n",
    "                    dense_x[f\"x_{depth_idx}_{depth_idx}\"] = output\n",
    "                else:\n",
    "                    dense_l_i = depth_idx + layer_idx\n",
    "                    cat_features = [dense_x[f\"x_{idx}_{dense_l_i}\"] for idx in range(depth_idx + 1, dense_l_i + 1)]\n",
    "                    cat_features = torch.cat(cat_features + [features[dense_l_i + 1]], dim=1)\n",
    "                    dense_x[f\"x_{depth_idx}_{dense_l_i}\"] = self.blocks[f\"x_{depth_idx}_{dense_l_i}\"](\n",
    "                        dense_x[f\"x_{depth_idx}_{dense_l_i-1}\"], cat_features\n",
    "                    )\n",
    "        dense_x[f\"x_{0}_{self.depth}\"] = self.blocks[f\"x_{0}_{self.depth}\"](dense_x[f\"x_{0}_{self.depth-1}\"])\n",
    "        fpn_input = [features[0]]+[dense_x[f\"x_{0}_{i}\"] for i in range(self.depth)]\n",
    "        fpn_out = self.fpn(*fpn_input[::-1])\n",
    "        fpn_out = F.interpolate(fpn_out,scale_factor=4,mode='bilinear')\n",
    "        return torch.cat((dense_x[f\"x_{0}_{self.depth}\"], fpn_out), dim=1)\n",
    "\n",
    "class UnetPlusPlus_with_ASPP_FPN(SegmentationModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_name: str = \"resnet34\",\n",
    "            encoder_depth: int = 5,\n",
    "            encoder_weights: Optional[str] = \"imagenet\",\n",
    "            decoder_use_batchnorm: bool = True,\n",
    "            decoder_channels: List[int] = (256, 128, 64, 32, 16),\n",
    "            decoder_attention_type: Optional[str] = None,\n",
    "            segmentation_channels: int = 32,\n",
    "            in_channels: int = 3,\n",
    "            classes: int = 2,\n",
    "            activation: Optional[Union[str, callable]] = None,\n",
    "            aux_params: Optional[dict] = None,\n",
    "            atrous_rates: Tuple = (6, 12 ,18),\n",
    "        ):\n",
    "            super().__init__()\n",
    "\n",
    "            self.encoder = get_encoder(\n",
    "                encoder_name,\n",
    "                in_channels=in_channels,\n",
    "                depth=encoder_depth,\n",
    "                weights=encoder_weights,\n",
    "            )\n",
    "\n",
    "            self.decoder = UnetPlusPlus_with_ASPP_FPN_Decoder(\n",
    "                encoder_channels=self.encoder.out_channels,\n",
    "                decoder_channels=decoder_channels,\n",
    "                aspp_out_channels=self.encoder.out_channels[-1],\n",
    "                atrous_rates=atrous_rates,\n",
    "                segmentation_channels=segmentation_channels,\n",
    "                n_blocks=encoder_depth,\n",
    "                use_batchnorm=decoder_use_batchnorm,\n",
    "                center=True if encoder_name.startswith(\"vgg\") else False,\n",
    "                attention_type=decoder_attention_type,\n",
    "            )\n",
    "\n",
    "            self.segmentation_head = SegmentationHead(\n",
    "                in_channels=decoder_channels[-1]+segmentation_channels*4,\n",
    "                out_channels=classes,\n",
    "                activation=activation,\n",
    "                kernel_size=3,\n",
    "            )\n",
    "\n",
    "            if aux_params is not None:\n",
    "                self.classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)\n",
    "            else:\n",
    "                self.classification_head = None\n",
    "\n",
    "            self.name = \"unetplusplus-with-aspp-fpn-{}\".format(encoder_name)\n",
    "            self.initialize()\n",
    "\n",
    "class LitModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = cfg.data.batch_size\n",
    "        self.cfg = cfg\n",
    "        self.cfg_optimizer = self.cfg.train.optimizer\n",
    "        self.cfg_scheduler = self.cfg.train.scheduler\n",
    "        self.cfg_scheduler.epochs = cfg.train.epochs\n",
    "        self.learning_rate = self.cfg.train.optimizer.learning_rate\n",
    "        self.weight_decay = self.cfg.train.optimizer.weight_decay\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = build_from_config(cfg.model)\n",
    "\n",
    "        self.loss_fn = self._init_loss_fn()\n",
    "\n",
    "        # self.dice_soft, self.dice_th = self._init_metric_fn()\n",
    "\n",
    "    def _init_loss_fn(self):\n",
    "        return SymmetricLovaszLoss(\"binary\")\n",
    "\n",
    "    # def _init_metric_fn(self):\n",
    "    #     return Dice_soft(), Dice_threshold()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Setup the optimizer\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=self.parameters(), lr=self.cfg_optimizer.learning_rate, weight_decay=self.cfg_optimizer.weight_decay)\n",
    "\n",
    "        # Setup the scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                    step_size=self.cfg_scheduler.step_size,\n",
    "                                                    gamma=self.cfg_scheduler.gamma)\n",
    "\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\"}]\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(images)\n",
    "\n",
    "    def training_step(self, batch: Dict, batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Dict, batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch: Dict, batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"test\")\n",
    "\n",
    "    def predict_step(self, batch: Dict, batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"test\")\n",
    "\n",
    "    def _step(self, batch: Dict[str, torch.Tensor], step: str) -> torch.Tensor:\n",
    "        images, masks = batch[\"image\"].float(), batch[\"mask\"].int()\n",
    "        outputs = self(images)\n",
    "\n",
    "        loss = self.loss_fn(outputs, masks)\n",
    "        # dice_soft = self.dice_soft(outputs, masks)\n",
    "        # dice_th = self.dice_th(outputs, masks)\n",
    "        dice_soft = Dice_soft_func(outputs, masks)\n",
    "        dice_th, best_th = Dice_threshold_func(outputs, masks)\n",
    "\n",
    "        self.log(f\"{step}_loss\", loss, sync_dist=True)\n",
    "        self.log(f\"{step}_dice_soft\", dice_soft, sync_dist=True)\n",
    "        self.log(f\"{step}_dice_th\", dice_th, sync_dist=True)\n",
    "\n",
    "        if step == \"test\":\n",
    "            self.log(f\"{step}_best_th\", best_th)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @classmethod\n",
    "    def load_eval_checkpoint(cls, checkpoint_path: str, device: str) -> nn.Module:\n",
    "        module = cls.load_from_checkpoint(checkpoint_path=checkpoint_path).to(device)\n",
    "        module.eval()\n",
    "\n",
    "        return module\n",
    "\n",
    "    def load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True):\n",
    "        return super().load_state_dict(state_dict, strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = EasyConfig()\n",
    "cfg.load(CONFIG_YAML_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_module = LitDataModule(\n",
    "#     data_frame=pd.read_csv(\"./input/hubmap-organ-segmentation/test.csv\"),\n",
    "#     spatial_size=512,\n",
    "#     batch_size=1,\n",
    "#     num_workers=1,\n",
    "#     )\n",
    "\n",
    "# data_module.setup()\n",
    "\n",
    "module = LitModule(cfg).to(device)\n",
    "module.load_state_dict(torch.load('./working/UnetPlusPlus_with_ASPP_FPN_efficientnet-b6_512_0.pth'))\n",
    "module.eval()\n",
    "\n",
    "# test_dl = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, size=512, limit_size=512*2):\n",
    "        self.dataframe = dataframe\n",
    "        self.size = size\n",
    "        self.limit_size = limit_size\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(os.path.join(DATA,f'{self.dataframe.loc[idx, [\"id\"]][0]}.tiff'))\n",
    "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h = img.shape[0]\n",
    "        w = img.shape[1]\n",
    "        data = dict()\n",
    "        if (h < self.limit_size) or (w < self.limit_size):\n",
    "            flag = -1\n",
    "            img = cv2.resize(img, (self.size,self.size))\n",
    "            result = np.vstack(np.hstack(img,img),np.hstack(img,img))\n",
    "        else:\n",
    "            flag = 1\n",
    "            img = cv2.resize(img, (self.limit_size,self.limit_size))\n",
    "        img = ToTensorV2().apply(img)\n",
    "\n",
    "        return {\"image\":img, \"h\":h, \"w\":w, \"flag\": flag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./input/hubmap-organ-segmentation/test_images/62.tiff\")\n",
    "tmp_mask = rle2mask(train_df.loc[302, \"rle\"], (3000,3000))\n",
    "plt.imshow(img)\n",
    "plt.imshow(tmp_mask, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./input/hubmap-organ-segmentation/test_images/10044.tiff\")\n",
    "tmp_mask = rle2mask(train_df.loc[0, \"rle\"], (3000,3000))\n",
    "plt.imshow(img)\n",
    "plt.imshow(tmp_mask, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = EasyDataset(pd.read_csv(TEST_CSV))\n",
    "test_dl = DataLoader(test_ds, 1, False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "names,preds = [],[]\n",
    "\n",
    "tmp_pd = pd.read_csv(TEST_CSV)\n",
    "\n",
    "TH = 0.65\n",
    "index = 0\n",
    "for i, batch in enumerate(test_dl):\n",
    "    if index == 3:\n",
    "        break\n",
    "    else:\n",
    "        small_batch = batch[\"image\"].squeeze(0).permute(1,2,0).reshape(2,512,2,512,3).permute(0,2,1,3,4).contiguous().view(-1, 512, 512,3)\n",
    "        one = small_batch.view(2,2,512,512,3).permute(0,2,1,3,4).reshape(1024,1024,3)\n",
    "        one = cv2.resize(one.numpy(), (batch[\"h\"].numpy()[0], batch[\"w\"].numpy()[0]))\n",
    "        mask = module(small_batch.permute(0,3,1,2).float().to(device))\n",
    "        mask = torch.nn.Sigmoid()(mask)\n",
    "        mask = mask.view(2,2,512,512,1).permute(0,2,1,3,4).reshape(1024,1024)\n",
    "        mask[mask<=TH]=0\n",
    "        mask[mask>TH]=1\n",
    "        mask = cv2.resize(mask.cpu().detach().numpy()*255, (batch[\"h\"].numpy()[0], batch[\"w\"].numpy()[0]))\n",
    "        plt.imshow(one, vmin=0, vmax=255)\n",
    "        plt.imshow(mask, alpha=0.2)\n",
    "\n",
    "        rle = mask2rle(mask)\n",
    "        names.append(tmp_pd.loc[i, \"id\"])\n",
    "        preds.append(rle)\n",
    "\n",
    "        del mask, small_batch, one\n",
    "        gc.collect()\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':names,'rle':preds})\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yzy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f91abce07d273583bd7a75d5496090b46b21fc7508d2a3384552f28a6b2401e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
